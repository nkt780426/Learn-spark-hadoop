{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe11db7-0c51-4e48-bf96-2b4790085806",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e339bad7-9a29-43f1-99de-de9ef36886b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d010e9cd-a2d3-4277-9729-0f82e901f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7f5aad-a559-426b-bdcf-25d877a74103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8799af60-2602-4c06-89be-9320d1dd6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "# spark.master: địa chỉ và giao thức kết nối của cluster manager mà các worker và client dùng để submit job và chỉ dùng để submit job.\n",
    "    # Do cụm spark này được xây trên docker compose không có cluster manager bên ngoài nên sử dụng cluster manager mặc định trong spark => Giao thức kết nối là spark\n",
    "# spark.driver.host: địa chỉ ip chính xác mà các client và excutor dùng để kết nối đến spark driver. \n",
    "    # Do đoạn code này được submit lên cluster manager nằm trong chính contianer driver nên có thể để mặc định là localhost. \n",
    "    # Tuy nhiên trong môi trường k8s, code được submit lên api server container chứ không phải spark-driver container nên phải chỉ rõ. Tốt nhất là chỉ rõ trong mọi trường hợp.\n",
    "# spark.driver.bindaddress: địa chỉ mà spark-driver chấp nhận gói tin ip. (cài này thêm vào thì lỗi)\n",
    "    # Nếu bạn muốn chỉ đính danh chỉ 1 máy có thể kết nối đến spark-driver thì cấu hình chính xác địa chỉ ip của máy đó\n",
    "    # Nếu để là 0.0.0.0 thì spark-driver chấp nhận gói tin ip của tất cả các máy (hiển nhiên vẫn phải mở thêm port)\n",
    "# .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test-1\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589107d6-a5b4-4c52-8347-3da9beab20b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fe00ffc222b2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1142366d50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24742ce-5bc9-4de5-8008-a7454598dcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'fe00ffc222b2'),\n",
       " ('spark.driver.port', '35547'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.submitTime', '1726116014205'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/jovyan/notebooks/spark-warehouse'),\n",
       " ('spark.app.name', 'test-1'),\n",
       " ('spark.driver.bindAddress', '0.0.0.0'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.master', 'spark://spark-master:7077'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'app-20240912044015-0000'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.app.startTime', '1726116014398'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f75c54-e99c-4c76-9cb6-0024e323776a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# How to create RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838f9fb-df53-435b-a348-85b83f16c1c4",
   "metadata": {},
   "source": [
    "## Ví dụ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "233c6e54-5520-4622-8426-2cd502dbca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "# Tạo RDD bằng cách phân mảnh mảng numbers\n",
    "rdd = spark.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93648b53-5437-4f16-a693-df2e7e9843ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af74f101-ba3a-42f4-8b70-166fde171284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of tuples\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35), (\"Alice\", 40)]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2d93b8e-79b7-4b96-af7a-72db4d66871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALl elements of the RDD:  [('Alice', 25), ('Bob', 30), ('Charlie', 35), ('Alice', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Collect action: Retrieve all elements of the RDD\n",
    "print(\"ALl elements of the RDD: \", rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fa5fa-76a9-4948-82c9-a5e68fb49abd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RDDs Operation: actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e62f4c85-8534-4a0a-b87c-fe07171d735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of elements in rdd:  4\n"
     ]
    }
   ],
   "source": [
    "# RDD support rất nhiều action để có thể show kết quả trong quá trình xử lý tương tác\n",
    "# count(): Đếm số phần tử trong RDD\n",
    "count = rdd.count()\n",
    "print(\"The total number of elements in rdd: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f68a9599-9839-4a3f-ab20-5fb668da0f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of the rdd:  ('Alice', 25)\n"
     ]
    }
   ],
   "source": [
    "# first(): lấy phần tử đầu tiên của RDD\n",
    "first_element = rdd.first()\n",
    "print(\"The first element of the rdd: \", first_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dcdf28f7-50d8-4233-8d9e-b0633256a252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two elements of the rdd:  [('Alice', 25), ('Bob', 30)]\n"
     ]
    }
   ],
   "source": [
    "# take(n): lấy n phần tử đầu tiên từ RDD\n",
    "print(\"The first two elements of the rdd: \", rdd.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30fc059d-10e9-4382-821f-ada23a921fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach(func**): cho phép apply 1 function lên các phần tử trong RDD\n",
    "rdd.foreach(lambda x: print(x))  # In từng phần tử x ra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0057e0-3fee-4154-b877-b1b9c1a26271",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RDDs Operation: transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6852d583-36e8-4a2b-b5ec-8b7c15b004ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformations: convert name ra in hoa\n",
    "mapped_rdd  = rdd.map(lambda x: (x[0].upper(), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13cd7834-4eee-403c-a64c-61ae995749c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd with uppercase name:  [('ALICE', 25), ('BOB', 30), ('CHARLIE', 35), ('ALICE', 40)]\n"
     ]
    }
   ],
   "source": [
    "result = mapped_rdd.collect()\n",
    "print(\"rdd with uppercase name: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ab1b0bf-99b6-4cfe-b9f0-5c51c7c49898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 35), ('Alice', 40)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lọc các phần tử rừ RDD cũ sang RDD mới: do RDD là không thay đổi mỗi khi được tạo ra\n",
    "fillted_rdd = rdd.filter(lambda x:x[1] > 30)\n",
    "fillted_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4915ba7e-836d-4ebc-9a4d-4aa01834169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 65), ('Bob', 30), ('Charlie', 35)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey(): transform groups elements có cùng 1 key vào\n",
    "# Ví dụ đếm số người có cùng tuổi trong rdd trên\n",
    "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db0f1482-2ea6-4d4e-a74a-240a42b33d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 40), ('Charlie', 35), ('Bob', 30), ('Alice', 25)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SortBy transformation: Sort the RDD by age in descending order\n",
    "sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063de675-3702-4f1a-957d-425ff2060ea1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save RDDs to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4fc0aef-14c5-4d48-9a48-861b25d67123",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o439.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1620)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1620)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1606)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1606)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 86.0 failed 4 times, most recent failure: Lost task 1.3 in stage 86.0 (TID 112) (172.18.0.5 executor 0): ExitCodeException exitCode=1: chmod: changing permissions of '/home/jovyan/notebooks/output/_temporary/0/_temporary/attempt_202409120326286597368956697215441_0240_m_000001_3': Operation not permitted\n\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:513)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: ExitCodeException exitCode=1: chmod: changing permissions of '/home/jovyan/notebooks/output/_temporary/0/_temporary/attempt_202409120326286597368956697215441_0240_m_000001_3': Operation not permitted\n\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:513)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cẩn thận khi sử dụng lệnh saveAsTextFile và sử dụng đường dẫn tại local, spark sẽ lưu nó vào cụm của mình. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Spark chỉ dùng để xử lý dữ liệu không dùng để chứa dữ liệu sau xử lý. Ở đây làm vậy vì đường dẫn này là volumn của spark\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Lưu RDD vào thư mục output\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/notebooks/output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3425\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   3423\u001b[0m     keyed\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mBytesToString())\u001b[38;5;241m.\u001b[39msaveAsTextFile(path, compressionCodec)\n\u001b[1;32m   3424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3425\u001b[0m     \u001b[43mkeyed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o439.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1620)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1620)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1606)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1606)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 86.0 failed 4 times, most recent failure: Lost task 1.3 in stage 86.0 (TID 112) (172.18.0.5 executor 0): ExitCodeException exitCode=1: chmod: changing permissions of '/home/jovyan/notebooks/output/_temporary/0/_temporary/attempt_202409120326286597368956697215441_0240_m_000001_3': Operation not permitted\n\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:513)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: ExitCodeException exitCode=1: chmod: changing permissions of '/home/jovyan/notebooks/output/_temporary/0/_temporary/attempt_202409120326286597368956697215441_0240_m_000001_3': Operation not permitted\n\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:513)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Cẩn thận khi sử dụng lệnh saveAsTextFile và sử dụng đường dẫn tại local, spark sẽ lưu nó vào cụm của mình. \n",
    "# Spark chỉ dùng để xử lý dữ liệu không dùng để chứa dữ liệu sau xử lý. Ở đây làm vậy vì đường dẫn này là volumn của spark\n",
    "# Lưu RDD vào thư mục output\n",
    "rdd.saveAsTextFile(\"/home/jovyan/notebooks/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6ccc5-fced-4965-8a28-c4a344602a58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Using Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a02b20-0e52-46c7-b244-5755908dabe4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Read csv file to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbfe121c-7c28-4021-8f67-7448f66d9c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,category,quantity,price\n",
      "1,iPhone 12,Electronics,10,899.99\n",
      "2,Nike Air Max 90,Clothing,25,119.99\n",
      "3,KitchenAid Stand Mixer,Home Appliances,5,299.99\n",
      "4,The Great Gatsby,Books,50,12.99\n",
      "5,L'Oreal Paris Mascara,Beauty,100,9.99\n",
      "6,Yoga Mat,Sports,30,29.99\n",
      "7,Samsung 4K Smart TV,Electronics,8,799.99\n",
      "8,Levi's Jeans,Clothing,15,49.99\n",
      "9,Dyson Vacuum Cleaner,Home Appliances,3,399.99\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -10 ./data/products.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9939f40-d630-4eb2-bb92-5ac4fcafc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = './data/products.csv'\n",
    "# df = spark.read.csv(csv_file_path, header=True)\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/data/products.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c5ef0b-f470-48e1-a66f-da11f75ba5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1a4422-b71c-43f8-8349-ef9b5e6c52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Không ai để schema là auto cả mà thường phải set\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf255fbb-b015-411a-a7d8-c7b843af1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(name='id', dataType=IntegerType(), nullable=True),\n",
    "    StructField(name='name', dataType=StringType(), nullable=True),\n",
    "    StructField(name='category', dataType=StringType(), nullable=True),\n",
    "    StructField(name='quantity', dataType=IntegerType(), nullable=True),\n",
    "    StructField(name='price', dataType=DoubleType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31994a99-1e52-452f-bc4c-ec0f17a94171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(\"hdfs://namenode:9000/output/products.json\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0309c46b-44c6-41e8-a289-e8e8be336fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = './data/products.csv'\n",
    "df = spark.read.csv(csv_file_path, header=True, schema=schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7061eb-20fb-4dff-aebf-f22e14cdb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Để spark dự đoán loại dữ liệu tự động\n",
    "csv_file_path = './data/products.csv'\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bdf863-0e9b-4ba3-a8f1-601950d0bf86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Read JSON file to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37718d-6bd6-4d65-a394-d54699718777",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Single Line JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a15e0b2-0f05-4aca-bbf7-074e251640ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":1,\"name\":\"iPhone 12\",\"category\":\"Electronics\",\"quantity\":10,\"price\":899.99}\n",
      "{\"id\":2,\"name\":\"Nike Air Max 90\",\"category\":\"Clothing\",\"quantity\":25,\"price\":119.99}\n",
      "{\"id\":3,\"name\":\"KitchenAid Stand Mixer\",\"category\":\"Home Appliances\",\"quantity\":5,\"price\":299.99}\n",
      "{\"id\":4,\"name\":\"The Great Gatsby\",\"category\":\"Books\",\"quantity\":50,\"price\":12.99}\n",
      "{\"id\":5,\"name\":\"L'Oreal Paris Mascara\",\"category\":\"Beauty\",\"quantity\":100,\"price\":9.99}\n",
      "{\"id\":6,\"name\":\"Yoga Mat\",\"category\":\"Sports\",\"quantity\":30,\"price\":29.99}\n",
      "{\"id\":7,\"name\":\"Samsung 4K Smart TV\",\"category\":\"Electronics\",\"quantity\":8,\"price\":799.99}\n",
      "{\"id\":8,\"name\":\"Levi's Jeans\",\"category\":\"Clothing\",\"quantity\":15,\"price\":49.99}\n",
      "{\"id\":9,\"name\":\"Dyson Vacuum Cleaner\",\"category\":\"Home Appliances\",\"quantity\":3,\"price\":399.99}\n",
      "{\"id\":10,\"name\":\"Harry Potter Series\",\"category\":\"Books\",\"quantity\":20,\"price\":15.99}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -10 data/products_singleline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122c1d31-67e2-4c44-94df-8ef20ca336d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = './data/products_singleline.json'\n",
    "df = spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec00a78-9270-4c0d-83cf-b2f87a7ee40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be4c13-dcbb-4065-8109-b2f5f77eae58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Multi line JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d386fb82-dd33-441a-84c0-b9171e1c90af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"name\": \"iPhone 12\",\n",
      "    \"category\": \"Electronics\",\n",
      "    \"quantity\": 10,\n",
      "    \"price\": 899.99\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"name\": \"Nike Air Max 90\",\n",
      "    \"category\": \"Clothing\",\n",
      "    \"quantity\": 25,\n",
      "    \"price\": 119.99\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"name\": \"KitchenAid Stand Mixer\",\n",
      "    \"category\": \"Home Appliances\",\n",
      "    \"quantity\": 5,\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -20 data/products_multiline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fef1431a-1e1e-40ee-8808-58edd0eda1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path='./data/products_multiline.json'\n",
    "df = spark.read.json(json_file_path, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05589c16-0435-469d-97e6-313ff3fc9161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "|         Sports|  6|            Yoga Mat| 29.99|      30|\n",
      "|    Electronics|  7| Samsung 4K Smart TV|799.99|       8|\n",
      "|       Clothing|  8|        Levi's Jeans| 49.99|      15|\n",
      "|Home Appliances|  9|Dyson Vacuum Cleaner|399.99|       3|\n",
      "|          Books| 10| Harry Potter Series| 15.99|      20|\n",
      "|         Beauty| 11|        MAC Lipstick| 16.99|      75|\n",
      "|         Sports| 12|Adidas Running Shoes| 59.99|      22|\n",
      "|    Electronics| 13|       PlayStation 5|499.99|      12|\n",
      "|       Clothing| 14|   Hooded Sweatshirt| 34.99|      10|\n",
      "|Home Appliances| 15|        Coffee Maker| 89.99|       7|\n",
      "|          Books| 16|To Kill a Mocking...|  9.99|      15|\n",
      "|         Beauty| 17|        Skincare Set| 49.99|      50|\n",
      "|         Sports| 18|           Yoga Ball| 19.99|      18|\n",
      "|    Electronics| 19|Sony Noise-Cancel...|299.99|       6|\n",
      "|       Clothing| 20|        Puma T-shirt| 19.99|      40|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048b5fa-b2df-470b-b3b0-5d3ec4097b82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Read parquet file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96e782bc-f7f7-42ab-af0a-f95ea43b00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_path = './data/products.parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7988b2d7-d874-4457-add7-d7b9ce756bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082e7e1-6c58-4937-9550-f32ffda1827f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataframe Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44061bf8-1ef8-43b6-856c-0be2ed051a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "Initial Dataframe: \n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file_path = './data/stocks.txt'\n",
    "df = spark.read.csv(data_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display schema of Dataframe\n",
    "df.printSchema()\n",
    "\n",
    "# Show the initial Dataframe\n",
    "print('Initial Dataframe: ')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf52c29e-571b-4315-9c12-ccdadfd10b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Columns: \n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "selected_columns = df.select('id', 'name', 'price')\n",
    "print(\"Selected Columns: \")\n",
    "selected_columns.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10f417c0-12be-4839-ad33-6f607ac53a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered_data:  12\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| id|          name|   category|quantity|price|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "|  6|    Nike Shoes|   Clothing|      30|99.99|\n",
      "|  7|  Adidas Shoes|   Clothing|      25|89.99|\n",
      "| 12|        Apples|       Food|     100|  0.5|\n",
      "| 13|       Bananas|       Food|     150| 0.25|\n",
      "| 14|       Oranges|       Food|     120| 0.75|\n",
      "| 15|Chicken Breast|       Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|       Food|      30| 5.99|\n",
      "| 24|    Laptop Bag|Accessories|      25|29.99|\n",
      "| 25|      Backpack|Accessories|      30|24.99|\n",
      "| 28|         Jeans|   Clothing|      30|59.99|\n",
      "| 29|       T-shirt|   Clothing|      50|14.99|\n",
      "| 30|      Sneakers|   Clothing|      40|79.99|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter: apply 1 điều kiện gì đó để lọc row)\n",
    "filtered_data = df.filter(df.quantity > 20)\n",
    "print(\"Filtered_data: \", filtered_data.count())\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b5926fe-fd8f-4336-b530-1ef4c1ac0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped and Aggregated Data: \n",
      "+-----------+-------------+------------------+\n",
      "|   category|sum(quantity)|        avg(price)|\n",
      "+-----------+-------------+------------------+\n",
      "|       Food|          450|2.2960000000000003|\n",
      "|     Sports|           35|             34.99|\n",
      "|Electronics|           98| 586.6566666666665|\n",
      "|   Clothing|          200|  99.2757142857143|\n",
      "|  Furniture|           41|            141.99|\n",
      "|Accessories|           55|             27.49|\n",
      "+-----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Aggregations(tập hợp)\n",
    "grouped_data = df.groupby('category').agg({'quantity': 'sum', 'price': 'avg'})\n",
    "print('Grouped and Aggregated Data: ')\n",
    "grouped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e9b073a-cca4-40c9-a1b1-d83bb155224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Data: \n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "| id|            name|   category|quantity|  price|   category|\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|Electronics|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|Electronics|\n",
      "|  3|            iPad|Electronics|      15| 499.99|Electronics|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|Electronics|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|Electronics|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|   Clothing|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|   Clothing|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|Electronics|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|Electronics|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|  Furniture|\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join Dataframes\n",
    "df2 = df.select('id', 'category').limit(10)\n",
    "joined_data = df.join(df2, 'id', 'inner')\n",
    "print('Joined Data: ')\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e140696-ff09-49a9-8171-3be92abd6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Data: \n",
      "+---+--------------+-----------+--------+-----+\n",
      "| id|          name|   category|quantity|price|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| 13|       Bananas|       Food|     150| 0.25|\n",
      "| 12|        Apples|       Food|     100|  0.5|\n",
      "| 14|       Oranges|       Food|     120| 0.75|\n",
      "| 15|Chicken Breast|       Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|       Food|      30| 5.99|\n",
      "| 29|       T-shirt|   Clothing|      50|14.99|\n",
      "| 19|      Yoga Mat|     Sports|      20|19.99|\n",
      "| 25|      Backpack|Accessories|      30|24.99|\n",
      "| 24|    Laptop Bag|Accessories|      25|29.99|\n",
      "| 20|  Dumbbell Set|     Sports|      15|49.99|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Sorted Data Streaming: \n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "| 26|          Camera|Electronics|      10| 599.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "| 17|  Leather Jacket|   Clothing|      15| 199.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 18|     Winter Coat|   Clothing|      10| 149.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort: Arrange rows based on one or more columns\n",
    "sorted_data = df.orderBy('price')\n",
    "print('Sorted Data: ')\n",
    "sorted_data.show(10)\n",
    "\n",
    "from pyspark.sql.functions import col, desc\n",
    "sorted_data = df.orderBy(col('price').desc(), col('id').desc())\n",
    "print('Sorted Data Streaming: ')\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd829a6f-a01b-40f3-bf44-917edea96755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Product Categories: \n",
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|       Food|\n",
      "|     Sports|\n",
      "|Electronics|\n",
      "|   Clothing|\n",
      "|  Furniture|\n",
      "|Accessories|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct: Get unique rows\n",
    "distinct_rows = df.select('category').distinct()\n",
    "print('Distinct Product Categories: ')\n",
    "distinct_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1564258f-4b88-49b9-aa78-47a7e1e68ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped Columns: \n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop: Remove specified columns\n",
    "dropped_columns = df.drop(\"quantity\", \"category\")\n",
    "print(\"Dropped Columns: \")\n",
    "dropped_columns.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "024f2c63-c1cd-4f88-b21f-4789170d8024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with new column: \n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "| id|            name|   category|quantity|  price|revenue|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99| 8999.9|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|6499.95|\n",
      "|  3|            iPad|Electronics|      15| 499.99|7499.85|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|6399.92|\n",
      "|  5|           LG TV|Electronics|      10| 699.99| 6999.9|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99| 2999.7|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|2249.75|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|1799.88|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99| 3999.8|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99| 2499.9|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WithColumn: Add new calculated columns\n",
    "df_with_new_column = df.withColumn(\"revenue\", df.quantity * df.price)\n",
    "print('Dataframe with new column: ')\n",
    "df_with_new_column.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9cf96a3-3884-451d-8bc9-e51a22ce4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with Aliased Column: \n",
      "+---+----------------+-----------+--------+-------------+\n",
      "| id|            name|   category|quantity|product_price|\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "|  1|          iPhone|Electronics|      10|       899.99|\n",
      "|  2|         Macbook|Electronics|       5|      1299.99|\n",
      "|  3|            iPad|Electronics|      15|       499.99|\n",
      "|  4|      Samsung TV|Electronics|       8|       799.99|\n",
      "|  5|           LG TV|Electronics|      10|       699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|        99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|        89.99|\n",
      "|  8| Sony Headphones|Electronics|      12|       149.99|\n",
      "|  9|Beats Headphones|Electronics|      20|       199.99|\n",
      "| 10|    Dining Table|  Furniture|      10|       249.99|\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alias\n",
    "df_with_alias = df.withColumnRenamed('price', 'product_price')\n",
    "print('Dataframe with Aliased Column: ')\n",
    "df_with_alias.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668f67f-db27-4e0e-8d00-927bcec2ea3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f2d0712-e0a0-4c0c-9556-2a3111bae8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "Initial Dataframe: \n",
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|        Jane Smith| 25|Female| 45000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "+------------------+---+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark sql là module của spark tạo điều kiện để quert dữ liệu có cấu trúc, bán cấu trúc bằng việc sử dụng lệnh sql thay vì khả năng spark dataframe\n",
    "# module này cung cấp khả năng xử lý theo batch và streaming cùng 1 lúc\n",
    "data_file_path = './data/persons.csv'\n",
    "df = spark.read.csv(data_file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "print('Initial Dataframe: ')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf5f857f-8a31-4135-b927-68aa3d1d2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Dataframe as a Temporary Table\n",
    "# Dùng để tạo 1 view tạm thời từ dataframe dùng cho truy vấn sql\n",
    "# Việc bắt buộc phải làm nêu muốn truy vấn bằng sql\n",
    "df.createOrReplaceTempView('my_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91bc7243-32c2-4632-a50b-aa4e07843348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lấy tất cả person có tuổi lớn hơn bằng 25\n",
    "result = spark.sql('SELECT * FROM my_table WHERE age > 25')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66de73ca-9f07-4daf-85d4-90cddec6a37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|gender|avg(salary)|\n",
      "+------+-----------+\n",
      "|Female|    52300.0|\n",
      "|  Male|    62100.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tính toán lương trung bình\n",
    "avg_salary_by_gender = spark.sql(\"SELECT gender, AVG(salary) FROM my_table GROUP BY gender\")\n",
    "avg_salary_by_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d4560cd-6e11-4461-a549-ce2a81bbadf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tạo temporary view\n",
    "df.createOrReplaceTempView('people')\n",
    "# Query temporary view\n",
    "result = spark.sql('SELECT * FROM people WHERE age > 25')\n",
    "result.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b669746-b46d-4a87-b960-1f22c8e490af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nếu temporary view tồn tại\n",
    "view_exits = spark.catalog.tableExists(\"people\")\n",
    "view_exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8eec3cad-3676-49ed-ab92-3f84530a811e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop 1 temporary view\n",
    "spark.catalog.dropTempView(\"people\")\n",
    "# Check nếu temporary view tồn tại\n",
    "view_exits = spark.catalog.tableExists(\"people\")\n",
    "view_exits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be817e-021a-4a2f-b54e-620c6f03e1be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Subquries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84ce09f0-f5b2-48ca-864a-e720ba75861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|   John|\n",
      "|  2|  Alice|\n",
      "|  3|    Bob|\n",
      "|  4|  Emily|\n",
      "|  5|  David|\n",
      "|  6|  Sarah|\n",
      "|  7|Michael|\n",
      "|  8|   Lisa|\n",
      "|  9|William|\n",
      "+---+-------+\n",
      "\n",
      "+----------+---+------+\n",
      "|department| id|salary|\n",
      "+----------+---+------+\n",
      "|        HR|  1| 60000|\n",
      "|        HR|  2| 55000|\n",
      "|        HR|  3| 58000|\n",
      "|        IT|  4| 70000|\n",
      "|        IT|  5| 72000|\n",
      "|        IT|  6| 68000|\n",
      "|     Sales|  7| 75000|\n",
      "|     Sales|  8| 78000|\n",
      "|     Sales|  9| 77000|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dataframe\n",
    "employee_data = [\n",
    "    (1, \"John\"), (2, \"Alice\"), (3, \"Bob\"), (4, \"Emily\"),\n",
    "    (5, \"David\"), (6, \"Sarah\"), (7, \"Michael\"), (8, \"Lisa\"), (9, \"William\")\n",
    "]\n",
    "\n",
    "employees = spark.createDataFrame(employee_data, [\"id\", \"name\"])\n",
    "\n",
    "salary_data = [\n",
    "    (\"HR\", 1, 60000), (\"HR\", 2, 55000), (\"HR\", 3, 58000), \n",
    "    (\"IT\", 4, 70000), (\"IT\", 5, 72000), (\"IT\", 6, 68000), \n",
    "    (\"Sales\", 7, 75000), (\"Sales\", 8, 78000), (\"Sales\", 9, 77000)\n",
    "]\n",
    "\n",
    "salaries = spark.createDataFrame(salary_data, [\"department\", \"id\", \"salary\"])\n",
    "\n",
    "employees.show()\n",
    "salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13b4411a-bfac-47b4-ae4e-7e2257135fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temporary view\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "salaries.createOrReplaceTempView(\"salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c818efa4-fabe-492b-9f75-91c27e3a4363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  Emily|\n",
      "|  David|\n",
      "|Michael|\n",
      "|   Lisa|\n",
      "|William|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subquery to find employees with salaries above average\n",
    "result = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT name\n",
    "        FROM employees\n",
    "        WHERE id IN (\n",
    "            SELECT id\n",
    "            FROM salaries\n",
    "            WHERE salary > (SELECT AVG(salary) FROM salaries)\n",
    "        )\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb20101-aafd-4403-957a-432b6eecb6cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecea5876-7401-4d32-93c4-1f5be135eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8954e9e-c5c3-4a16-a3b8-3e95fd34183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+\n",
      "|department| id|salary|   name|\n",
      "+----------+---+------+-------+\n",
      "|     Sales|  7| 75000|Michael|\n",
      "|        IT|  6| 68000|  Sarah|\n",
      "|     Sales|  9| 77000|William|\n",
      "|        IT|  5| 72000|  David|\n",
      "|     Sales|  8| 78000|   Lisa|\n",
      "|        HR|  1| 60000|   John|\n",
      "|        HR|  3| 58000|    Bob|\n",
      "|        HR|  2| 55000|  Alice|\n",
      "|        IT|  4| 70000|  Emily|\n",
      "+----------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subquery to find employees with salaries above average\n",
    "employee_salary = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT salaries.*, employees.name\n",
    "        FROM salaries\n",
    "        LEFT JOIN employees on salaries.id = employees.id\n",
    "    \"\"\"\n",
    ")\n",
    "employee_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0957b64c-898d-4846-b456-32d6df80d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window specification\n",
    "# Windown func giúp thực hiện query trên 1 cửa sổ windown gồm nhiều row thay vì thực hiện trên từng row\n",
    "# Dataframe được chia thành các windown - partition\n",
    "windown_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75f5954d-98f2-4689-a910-5741f4aea126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+----+\n",
      "|department| id|salary|   name|rank|\n",
      "+----------+---+------+-------+----+\n",
      "|        HR|  1| 60000|   John|   1|\n",
      "|        HR|  3| 58000|    Bob|   2|\n",
      "|        HR|  2| 55000|  Alice|   3|\n",
      "|        IT|  5| 72000|  David|   1|\n",
      "|        IT|  4| 70000|  Emily|   2|\n",
      "|        IT|  6| 68000|  Sarah|   3|\n",
      "|     Sales|  8| 78000|   Lisa|   1|\n",
      "|     Sales|  9| 77000|William|   2|\n",
      "|     Sales|  7| 75000|Michael|   3|\n",
      "+----------+---+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_salary.withColumn(\"rank\", F.rank().over(windown_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1887e-d641-4e59-a08f-5931b9058e59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Đóng spark session sau khi hoàn thành xong công việc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e208d171-9c98-4f40-853f-c85d74a98119",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
